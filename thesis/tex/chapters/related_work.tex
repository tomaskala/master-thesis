\chapter{Related work}
\label{chap:related-work}

In this chapter, we provide a survey of literature relevant to our task. Addressed are works on the use of Markov Chain Monte Carlo methods for approximate inference, works on approximating the likelihood of state-space models by the particle filter, and on Approximate Bayesian Computation methods. We also give a section describing the use of the considered models in bioinformatics, focusing on molecular biology and genetics.

\section{Markov Chain Monte Carlo methods}
Monte Carlo methods can be described as a class of algorithms designed to simulate random samples from a distribution of interest, which itself is too complex to sample directly. Assuming that the probability density function of this distribution can be evaluated (at least up to a normalizing constant), Monte Carlo methods output a random sample approximately distributed according to the true distribution. \emph{Markov Chain} Monte Carlo (MCMC) methods employ a Markov chain designed so that its stationary distribution is the target. At least asymptotically, the samples are indeed distributed according to the desired distribution.

An attractive property is that the transition distribution of such chain need not resemble the target distribution even closely, and that the problem is relatively unaffected by the dimensionality. The downside is a difficulty to determine convergence --- for how long should a chain be ran in order to approximately reach the stationary distribution. In addition, one typically requires independent samples from the target distribution, which, however, the Markov chain samples are \emph{not}. Typically, one needs to ``thin'' the Markov chain samples by keeping every $n$th one to ensure their approximate independence.

Perhaps the best known MCMC algorithm is the Metropolis algorithm \citep{metropolis}, later improved by \cite{hastings}. Random samples are iteratively generated from the Markov chain transition distribution, called the proposal distribution in this context. Each such sample is then compared with the previous one, and accepted with a certain probability which ensures that the stationary distribution is indeed the target. The go-to reference for Monte Carlo methods is \cite{robert-casella}. A particularly appealing treatment of MCMC methods with applications towards physics and machine learning can be found in \cite{information-theory}.

There are of course many more MCMC algorithms. For our task, the Metropolis-Hastings algorithm is sufficient, since the main problem is in the likelihood estimation, and not in designing the best sampler possible.

\section{Parameter inference in state-space models}
Assuming that the state-space model (SSM) takes the form informally stated in \autoref{chap:introduction} and more formally given in \autoref{chap:inference}, if all the parameters of interest are changing in time, that is, the inference is about $x_t$ given $y_1, \ldots, y_t$, one arrives at the task of filtering.

If the transition distribution from state $x_t$ to state $x_{t+1}$ is linear in the states and corrupted by uncorrelated additive noise centered at 0, this task can be solved exactly by the Kalman filter \citep{kalman}. The resulting filter is then optimal with respect to the mean squared error. An especially nice overview of the Kalman filter connecting it with other linear statistical models is \cite{lds}.

Once the state transition becomes non-linear, as is typically the case, one can use various generalizations of the Kalman filter, such as the extended Kalman filter (EKF), which locally linearizes the transition distribution, or the unscented Kalman filter \citep{ukf}. These methods come without any optimality guarantees, though. The EKF additionally works best under a very mild non-linearity, due to its first-order approximation.

In recent years, the particle filter \citep{particle-filter} has become a popular alternative due to its particularly simple implementation, appealing asymptotic properties and the fact that it allows for the transition model to be arbitrarily non-linear. Since the particle filter is used later in \autoref{chap:inference}, we postpone a more detailed description there.

If, on the other hand, some of the unknown parameters are static, the task becomes more complex. Blindly applying an MCMC algorithm or any other approximation is not possible, as the likelihood function, on which such algorithms typically depend, cannot be evaluated. The paper \cite{andrieu} introduced the idea of using the particle filter to obtain an estimate of the likelihood, which has been shown in \cite{del-moral} to preserve the stationary distribution of the underlying Markov chain. The resulting algorithm is called \textit{Marginal Metropolis-Hastings}. A more recent overview can be found in the tutorial by \cite{schoen}.

\section{Approximate Bayesian Computation}
In its original formulation, the method of Approximate Bayesian Computation (ABC) provides a way to approximate the posterior distribution $p(\theta \mid y) \propto f(y \mid \theta) p(\theta)$, assuming that the prior $p(\cdot)$ is fully known, and that the likelihood $f(\cdot \mid \theta)$ can be sampled from, but not evaluated \citep{abc-old-old, abc-old}. A more recent treatment of ABC methods can be found in \cite{abc-recent}.

Briefly, ABC works by simulating a sample $\tilde{\theta}$ from the prior, substituting it to the likelihood, and generating pseudo-observations $\tilde{y}$. These are then compared to the real observations $y$, and if they are ``similar enough'', the sample $\tilde{\theta}$ is accepted. Otherwise, it is rejected. The posterior distribution of $\theta$ is then given in terms of these random samples $\tilde{\theta}$. This variant is referred to as the accept-reject ABC, for obvious reasons.

In this thesis, we apply the ABC method in place of the particle filter to allow for inference about the static parameter $\theta$ when the likelihood is not available. In addition, the use of ABC allows for a possibly misspecified observation model of the SSM, which is often the case, as one may not possess the necessary domain knowledge or computational power needed for the real model. Such a situation has been considered in \cite{jasra-time-series}, although only through the use of the accept-reject variant given above.

Since accepting a sufficient number of samples may take a long time, an idea is to measure the distance between the true and pseudo-observations through a kernel function. This formulation would not reject any samples --- instead, previously rejected samples would get assigned low weights. This has been investigated in \cite{dedecius}, along with a proposed way to automatically tune the kernel width. How to exactly apply the ABC method to our problem is addressed in \autoref{chap:abc} in detail.

\section{Applications to molecular biology}
Finally, we review works describing how the framework of SSMs and their parameter inference can be applied in the context of bioinformatics, focusing on problems of molecular biology and genetics.

The go-to reference for stochastic modelling in biology is \cite{wilkinson-book}. It contains a broad overview of applications of various probabilistic models to examples from molecular biology and chemistry. Included is a description of the Gillespie algorithm \cite{gillespie1, gillespie2} used to simulate chemical reactions, which we use in \autoref{chap:applications}.

A recent application of SSMs to molecular biology can be found in \cite{wilkinson}, where the authors use the particle filter to approximate the unknown likelihoods of various biological models. We implement these examples in \autoref{chap:applications} and compare them with the ABC approximation.

The paper \cite{bio1} models biological networks, such as gene regulatory networks or signalling pathways, by SSMs, and estimates their parameters. The static parameters of the model are viewed as dynamic states which, however, do not change in time. The unscented Kalman filter is then applied to estimate these ``dynamic'' parameters. Such approach is simple, as it does not require the use of MCMC algorithms, but comes without the appealing asymptotic properties of MCMC inference.

\cite{bio2}, \cite{bio3} and \cite{bio4} proceed in a similar fashion when estimating the parameters of various biochemical networks. The used models are only mildly non-linear, and so the extended Kalman filter is sufficient, again without any asymptotic guarantees of identifying the true parameters.

An interesting approach to learning the structure of a gene regulatory network from a gene expression time series can be found in \cite{bio5}. First, the particle filter is applied to learn the hidden states of the network. Once these hidden states are known, the LASSO regression is applied to learn a sparse representation of the regulatory network, since each gene is assumed to interact only with a small number of other genes.