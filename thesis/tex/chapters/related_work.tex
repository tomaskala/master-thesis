\chapter{Related work}
\label{chap:related-work}

In this chapter, we provide a survey of literature relevant to our task. Addressed will be works on the use of Markov Chain Monte Carlo methods for approximate inference, works on approximating the likelihood of state-space models by the particle filter, and on Approximate Bayesian Computation methods. We also provide a section describing the use of the considered models in bioinformatics, focusing on molecular biology and genetics.

\section{Markov Chain Monte Carlo methods}
Markov Chain Monte Carlo (MCMC) can be summarized as algorithms designed to simulate random samples from a distribution of interest, which itself is too complicated to sample directly. Assuming the probability density function of this distribution can be evaluated (at least to a multiplicative constant), MCMC methods work by designing a Markov chain whose stationary distribution is the target one.

An attractive property is that the transition distribution of such chain need not resemble the target distribution even closely, and that the problem is relatively unaffected by the dimensionality of the distribution. The downside is a difficulty to determine convergence --- for how long should a chain be ran in order to approximately reach this stationary distribution. In addition, one typically requires independent samples from the target distribution, which, however, the Markov chain samples are \emph{not}. Typically, one needs to ``thin'' the Markov chain samples by keeping every $n$th one to ensure their approximate independence.

Perhaps the best known MCMC algorithm is the Metropolis algorithm \citep{metropolis}, later improved by \cite{hastings}. Random samples are iteratively generated from the Markov chain transition distribution, called the proposal distribution in this context. Each such sample is then compared with the previous one, and accepted with a certain probability which ensures that the stationary distribution is indeed the target. The go-to reference for Monte Carlo methods is \cite{robert-casella}. A particularly appealing treatment of MCMC methods with applications towards physics and machine learning can be found in \cite{information-theory}.

\section{Parameter inference in state-space models}
Assuming that the state-space model (SSM) takes the form informally stated in \autoref{chap:introduction} and more formally given in \autoref{chap:inference}, if all the parameters of interest are changing in time, that is, the inference is about $x_t$ given $y_t$, one arrives at the task of filtering.

If the transition distribution from state $x_t$ to state $x_{t+1}$ is linear in the states and corrupted by uncorrelated additive noise with mean 0, this task can be solved exactly by the Kalman filter \citep{kalman}. The resulting filter is then optimal with respect to the mean squared error. An especially nice overview of the Kalman filter connecting it with other linear statistical models is \cite{lds}.

Once the state transition becomes non-linear, one can use various generalizations of the Kalman filter, such as the extended Kalman filter, which locally linearizes the transition distribution, or the unscented Kalman filter \citep{ukf}.

In recent years, though, the particle filter \citep{particle-filter} has become the most popular alternative due to its particularly simple implementation, appealing asymptotic properties and the fact that it allows for the transition model to be arbitrarily non-linear. The algorithm uses a relatively small number of random samples to approximate the distribution of $x_t$ given $y_1, \ldots, y_t$ at any given time $t$. Since the particle filter is used later in \autoref{chap:inference}, we postpone a more detailed description there.

On the other hand, if some of the unknown parameters are static, the task becomes more complex. Simply applying MCMC algorithms or other approximations is not possible, as the likelihood function, which is a part of the Metropolis-Hastings algorithm, cannot be evaluated. The paper \cite{andrieu} introduced the idea of using the particle filter to obtain an estimate of the likelihood, which has been shown in \cite{del-moral} to preserve the stationary distribution of the underlying Markov chain. The resulting algorithm is called \emph{Marginal Metropolis-Hastings}. A more recent overview can be found in the tutorial by \cite{schoen}.

\section{Approximate Bayesian Computation}
In its original formulation, the method of Approximate Bayesian Computation (ABC) provides a way to approximate the posterior distribution $p(\theta \mid y) \propto f(y \mid \theta) p(\theta)$, assuming that the prior $p(\cdot)$ is fully known, and that the likelihood $f(\cdot \mid \theta)$ can be sampled from, but not evaluated \citep{abc-old-old, abc-old}. A more recent overview of ABC methods can be found in \cite{abc-recent}.

Briefly, the ABC method works by simulating a sample $\hat{\theta}$ from the prior, substituting it to the likelihood, and generating pseudo-observations $\hat{y}$. These are then compared to the real observations $y$, and if they are ``similar enough'', the sample $\hat{\theta}$ is accepted. Otherwise, it is rejected. The posterior distribution of $\theta$ is then given in terms of these random samples $\hat{\theta}$. This variant is referred to as the accept-reject ABC, for obvious reasons.

In this thesis, we apply the ABC method in place of the particle filter to allow for inference about the static parameter $\theta$ when the likelihood is not available. In addition, the use of ABC allows for a possibly misspecified observation model of the SSM, which is often the case, as one may not possess the necessary domain knowledge or computational power needed for the real model. Such a situation has been considered in \cite{jasra-time-series}, although only through the use of the accept-reject variant given above.

Since accepting a sufficient number of samples may take a long time, an idea is to measure the distance between the true and pseudo-observations through a kernel function. This formulation would not reject any samples --- instead, they would get assigned a lower weight. This is considered for instance in \cite{dedecius}, along with a proposed way to automatically tune the kernel width. How to exactly apply the ABC method to our problem will be addressed in \autoref{chap:abc} in detail.

\section{Applications to molecular biology}
Finally, we review works describing how the framework of SSMs and the parameter inference in those can be applied in the context of bioinformatics, focusing on problems of molecular biology and genetics.

The go-to reference for stochastic modelling in biology is \cite{wilkinson-book}. It contains a broad overview of applications of various probabilistic models to examples from molecular biology and chemistry. Included is a description of the Gillespie algorithm \cite{gillespie1, gillespie2} used to simulate chemical reactions, which we will use in \autoref{chap:applications}.

A recent application of SSMs to molecular biology can be found in \cite{wilkinson}, where the authors use the particle filter to approximate the unknown likelihoods of various biological models. We will implement these examples in \autoref{chap:applications} and compare them with the ABC approximation.

The paper \cite{bio1} views biological networks such as gene regulatory networks or signalling pathways as SSMs, and estimates their parameters. The static parameters of the model are viewed as dynamic states which, however, do not change in time. The unscented Kalman filter is then applied to estimate these ``dynamic'' parameters. Such approach is simple, as it does not require the use of MCMC algorithms, but comes without the appealing asymptotical properties of MCMC inference.

\cite{bio2}, \cite{bio3} and \cite{bio4} proceed in a similar fashion when estimating the parameters of various biochemical networks. The used models are only mildly non-linear, and so the extended Kalman filter is sufficient, again without any asymptotical guarantees of identifying the true parameters, however.

An interesting approach to learning the structure of a gene regulatory network from a gene expression time series can be found in \cite{bio5}. First, the particle filter is applied to learn the hidden states of the network. Once these hidden states are known, the LASSO regression is applied to learn a sparse representation of the regulatory network, since each gene is assumed to interact only with a small number of other genes.