\chapter{Learning the parameters of a state-space model}
\label{chap:inference}

This chapter describes the state-space model (SSM) formulation we are working with. In \autoref{sec:ssm-definition}, we state our assumptions about the individual probability distributions. Then in \autoref{sec:parameter-inference}, we calculate the posterior distribution of the parameters of interest, and show that straightforward inference is not possible. Further on, we derive a sampler to approximate this distribution. By itself, this sampler is unusable, as it requires the evaluation of the model likelihood. To circumvent this, we introduce the particle filter in \autoref{sec:particle-filter}. This section gives the definition and some of the properties of the filter. Later in \autoref{sec:particle-filter-estimate} we show how to use the particle filter to estimate the likelihood, and argue that it does not affect the asymptotic properties of the sampler.

Most of this chapter is based on \cite{andrieu} and \cite{schoen}.



\section{State-Space Model definition} \label{sec:ssm-definition}
The state-space model, often also called the hidden Markov model (HMM) assumes a sequence of latent states $\left\{\bx_t\right\}_{t=0}^\infty \subseteq \R^{d_x}$ following a Markov chain, and a sequence of observed variables $\left\{\by_t\right\}_{t=1}^\infty \subseteq \R^{d_y}$. All involved distributions are parameterized by an unknown static parameter $\btheta \in \Theta \subset \R^d$.

For a fixed time $T \geq 1$, we use the shorthands $\bx_{0:T} = \left\{\bx_t\right\}_{t=0}^T$ and $\by_{1:T} = \left\{\by_t\right\}_{t=1}^T$.

The HMM formulation means that the joint distribution of $\bx_{0:T}$ and $\by_{1:T}$ factorizes, for any $T \geq 1$, into
\begin{equation}\label{eq:factorization}
p(\bx_{0:T}, \by_{1:T} \mid \btheta) = \sprior(\bx_0 \mid \btheta) \prod_{t = 1}^{T} \trans_t(\bx_t \mid \bx_{t-1}, \btheta) \obs_t(\by_t \mid \bx_t, \btheta),
\end{equation}
where $\sprior$ is the prior distribution over the initial state, $\trans_t$ is the transition distribution at time $t$ and $\obs_t$ is the observation model at time $t$.

The factorization \eqref{eq:factorization} can be written more clearly as
\begin{align*}
\bx_0 \mid \btheta & \sim \sprior(\cdot \mid \btheta), \\
\bx_t \mid \bx_{t-1}, \btheta & \sim \trans_t(\cdot \mid \bx_{t-1}, \btheta), \quad t = 1, \ldots, T, \\
\by_t \mid \bx_t, \btheta & \sim \obs_t(\cdot \mid \bx_t, \btheta), \quad t = 1, \ldots, T.
\end{align*}

Finally, in accordance with the Bayesian approach \citep{bayes}, we introduce a prior distribution $\pprior$ over the unknown parameters $\btheta$ quantifying our knowledge about $\btheta$ before observing any data. This allows us to state the full joint distribution
\begin{equation}\label{eq:full-joint}
p(\bx_{0:T}, \by_{1:T}, \btheta) = p(\bx_{0:T}, \by_{1:T} \mid \btheta) \pprior(\btheta).
\end{equation}
The corresponding graphical model is depicted in \autoref{fig:graphical-model}.
\begin{figure}[ht]
    \centering
    \begin{tikzpicture}
    % Style
    \tikzstyle{main}=[circle, minimum size = 10mm, thick, draw =black!80, node distance = 16mm]
    \tikzstyle{connect}=[-latex, thick]
    
    % Nodes X
    \node[main,shape=circle,draw=black](X0) at (1,4) {$\bx_0$};
    \node[main,shape=circle,draw=black](X1) at (3,4) {$\bx_1$};
    \node[main,shape=circle,draw=black](X2) at (5,4) {$\bx_2$};
    \node[](Xdots) at (7,4) {$\ldots$};
    \node[main,shape=circle,draw=black](XT) at (9,4) {$\bx_T$};
    
    % Node theta
    \node[](theta) at (7,2) {$\btheta$};
    
    % Nodes Y
    \node[main,shape=circle,draw=black,fill=black!20](Y1) at (3,0) {$\by_1$};
    \node[main,shape=circle,draw=black,fill=black!20](Y2) at (5,0) {$\by_2$};
    \node[](Ydots) at (7,0) {$\ldots$};
    \node[main,shape=circle,draw=black,fill=black!20](YT) at (9,0) {$\by_T$};

    % Edges XX
    \path [->] (X0) edge[connect] node[left] [above] {$\trans_1$} (X1);
    \path [->] (X1) edge[connect] node[left] [above] {$\trans_2$} (X2);
    \path [->] (X2) edge[connect] node[left] [above] {$\trans_3$} (Xdots);
    \path [->] (Xdots) edge[connect] node[left] [above] {$\trans_T$} (XT);
    
    % Edges XY
    \path [->] (X1) edge[connect] node[left] [left] {$\obs_1$} (Y1);
    \path [->] (X2) edge[connect] node[left] [left] {$\obs_2$} (Y2);
    \path [->] (XT) edge[connect] node[left] [left] {$\obs_T$} (YT);
    
    % Edges theta X
    \path [->] (theta) edge[connect] node[left] {} (X0);
    \path [->] (theta) edge[connect] node[left] {} (X1);
    \path [->] (theta) edge[connect] node[left] {} (X2);
    \path [->] (theta) edge[connect] node[left] {} (XT);
    
    % Edges theta Y
    \path [->] (theta) edge[connect] node[left] {} (Y1);
    \path [->] (theta) edge[connect] node[left] {} (Y2);
    \path [->] (theta) edge[connect] node[left] {} (YT);
    \end{tikzpicture}
    \caption{Graphical model describing the full joint distribution \eqref{eq:full-joint}. The shaded nodes denote the observed variables, white nodes represent the latent variables.}
    \label{fig:graphical-model}
\end{figure}



\section{Parameter inference} \label{sec:parameter-inference}
Given an observed sequence $\by_{1:T}$, Bayesian inference relies on the joint posterior density
\begin{equation}\label{eq:joint-posterior}
p(\btheta, \bx_{0:T} \mid \by_{1:T}) = \underbrace{p(\bx_{0:T} \mid \btheta, \by_{1:T})}_{\text{State inference}} \underbrace{p(\btheta \mid \by_{1:T})}_{\text{Parameter inference}}.
\end{equation}
Our primary interest is to perform inference about the static parameter $\btheta$. From \eqref{eq:joint-posterior}, it is clear that to infer about the hidden states $\bx_{0:T}$, one needs knowledge about $\btheta$, so even if the hidden states are of interest, inference about $\btheta$ is necessary. \autoref{sec:particle-filter-estimate} actually shows how to estimate $\bx_{0:T}$ as a by-product.


\paragraph{Bayesian inference}

To perform Bayesian inference about $\btheta$, we express the posterior of $\btheta$ by applying the Bayes theorem:
\begin{equation} \label{eq:posterior}
p(\btheta \mid \by_{1:T}) = \frac{p(\by_{1:T} \mid \btheta) \pprior(\btheta)}{\int p(\by_{1:T} \mid \btheta) \pprior(\btheta) \; \dx{\btheta}}.
\end{equation}

Evaluating the likelihood $p(\by_{1:T} \mid \btheta)$ requires marginalizing over $\bx_{0:T}$:
\begin{equation*}
p(\by_{1:T} \mid \btheta) = \int p(\bx_{0:T}, \by_{1:T} \mid \btheta) \; \dx{\bx_{0:T}},
\end{equation*}
where $p(\bx_{0:T}, \by_{1:T} \mid \btheta)$ is given in \eqref{eq:factorization}. Unless the SSM is linear and Gaussian, such $d_x(T+1)$-dimensional integral is intractable \citep{andrieu}.


\paragraph{Inference under tractable likelihood assumption}

For the time being, we proceed as if the likelihood was tractable. We derive a sampler for $\btheta$ and note which component cannot be evaluated due to the likelihood being present. \autoref{sec:particle-filter-estimate} then describes the necessary changes to allow circumventing the likelihood evaluation.

Often, the interest is not in the posterior $p(\btheta \mid \by_{1:T})$ itself, but on the expectation of some function $\phi$ w.r.t. this distribution, i.e. on
\begin{equation} \label{eq:posterior-integral}
\E_{p(\btheta \mid \by_{1:T})}[\phi(\btheta)] = \int \phi(\btheta) p(\btheta \mid \by_{1:T}) \; \dx{\btheta}.
\end{equation}
We use the Metropolis-Hastings algorithm \citep{metropolis, hastings} to obtain $M$ samples from $p(\btheta \mid \by_{1:T})$, denoted as $\btheta^{(m)},\ m = 1, \ldots, M$. The integral \eqref{eq:posterior-integral} is then approximated as the arithmetic mean
\begin{equation*}
\frac{1}{M} \sum_{m=1}^M \phi(\btheta^{(m)}).
\end{equation*}
An appealing property of the Metropolis-Hastings algorithm is that such arithmetic mean converges to \eqref{eq:posterior-integral} almost surely \citep{robert-casella}, i.e.
\begin{equation*}
\frac{1}{M} \sum_{m=1}^M \phi(\btheta^{(m)}) \xrightarrow{a.s} \int \phi(\btheta) p(\btheta \mid \by_{1:T}) \; \dx{\btheta},
\end{equation*}
where $\xrightarrow{a.s.}$ denotes almost sure convergence.

Finally, we note that if one is indeed interested in the distribution $p(\btheta \mid \by_{1:T})$ itself, it can be recovered by the empirical distribution
\begin{equation*}
\widehat{p}(\btheta \mid \by_{1:T}) = \frac{1}{M} \sum_{m=1}^M \delta_{\btheta^{(m)}}(\btheta),
\end{equation*}
where $\delta$ denotes the Dirac distribution. This estimate can be additionally smoothed using kernel methods \citep{kernel-smoothing}.


\paragraph{Metropolis-Hastings algorithm}
The Metropolis-Hastings algorithm is described in \autoref{alg:metropolis-hastings}. Although well-known, it is included for comparison with the variant introduced in \autoref{alg:marginal-metropolis-hastings}.

The target distribution is the parameter posterior $p(\btheta \mid \by_{1:T}) \propto p(\by_{1:T} \mid \btheta) \pprior(\btheta)$. In this case, it is not necessary to evaluate the normalizing constant, since it gets cancelled out.

The algorithm further requires a proposal distribution $\prop$. Similarly to the prior $\pprior$, it is problem-dependent, and must be selected carefully.

\begin{algorithm}[ht]
    \caption{Metropolis-Hastings}
    \label{alg:metropolis-hastings}
    \begin{algorithmic}[1]
        \Input $\text{Number of samples } M,\ \left\{\by_1, \ldots, \by_T\right\}$
        
        \State $\text{Initialize } \btheta^{(0)}.$
        
        \For{$m = 1\ \mathbf{to}\ M$}
            \State $\text{Sample } \btheta^\prime \sim \prop(\cdot \mid \btheta^{(m-1)}).$
            \State $\text{Calculate the aceptance probability } $ \begin{equation} \label{eq:acceptance-probability}
            \alpha = \min \left\{1, \frac{p(\by_{1:T} \mid \btheta^\prime) \pprior(\btheta^\prime)}{p(\by_{1:T} \mid \btheta^{(m-1)}) \pprior(\btheta^{(m-1)})} \frac{\prop(\btheta^{(m-1)} \mid \btheta^\prime)}{\prop(\btheta^\prime \mid \btheta^{(m-1)})} \right\}.
            \end{equation}
            \State $\text{Sample } u \sim \mathcal{U}(0,1).$
            \If {$u \leq \alpha$}
                \State $\btheta^{(m)} \gets \btheta^\prime$ \Comment{With probability $\alpha$, accept the proposed sample.}
            \Else
                \State $\btheta^{(m)} \gets \btheta^{(m-1)}$ \Comment{With probability $1 - \alpha$, reject the proposed sample.}
            \EndIf
        \EndFor
        
        \Output $\left\{ \btheta^{(1)}, \ldots, \btheta^{(M)} \right\}$
    \end{algorithmic}
\end{algorithm}

We see that the acceptance probability \eqref{eq:acceptance-probability} cannot be calculated, as it depends on the intractable likelihood $p(\by_{1:T} \mid \btheta)$. In \autoref{sec:particle-filter-estimate}, we give a modified variant of the Metropolis-Hastings algorithm, where the likelihood is approximated using the particle filter. The derivation of this filter is the content of the next section.



\section{The particle filter} \label{sec:particle-filter}
The particle filter \citep{particle-filter} is a method for approximating the filtering distribution $p(\bx_t \mid \by_{1:t}, \btheta)$ using a finite number of samples called particles. The algorithm is also known as sequential Monte Carlo or sequential importance sampling. The latter name sheds some light on how the method works, and it is exactly through importance sampling that the particle filter is derived.

\paragraph{Importance sampling}
Here we briefly review the basic idea behind importance sampling. For a more thorough treatment, the reader is referred to \cite{information-theory} or \cite{robert-casella}.

Consider a situation where the expectation of some function $\phi$ w.r.t. the distribution with density $p$,
\begin{equation} \label{eq:is-expectation}
\Phi \coloneqq \E_{p}[\phi(\bm{X})] = \int \phi(\bm{x}) p(\bm{x}) \; \dx{\bm{x}},
\end{equation}
is of interest. Assume that the integral is analytically intractable, and that one cannot generate samples from $p$ to approximate this expectation. Assume further that the density $p$ can be evaluated, at least up to a multiplicative constant, i.e. that it takes the form
\begin{equation*}
p(\bm{x}) = \frac{p^*(\bm{x})}{Z},
\end{equation*}
where $Z$ is an unknown normalizing constant, and $p^*$ can be evaluated. Such situation frequently arises in Bayesian statistics, where a posterior distribution of interest $p(\btheta \mid \bm{x}) = \frac{p(\bm{x} \mid \btheta) p(\btheta)}{\int p(\bm{x} \mid \btheta) p(\btheta) \; \dx{\btheta}}$ is given in terms of the Bayes theorem. The normalizing constant in the denominator is often unavailable in analytic form. However, the numerator can be evaluated.

Next, we introduce a (typically simpler) distribution with probability density $q(\bm{x}) = \frac{q^*(\bm{x})}{Z_Q}$ such that
\begin{enumerate}
    \item One can sample from $q$;
    \item One can evaluate $q^*$;
    \item $p(\bm{x}) > 0$ implies $q(\bm{x}) > 0$.
\end{enumerate}
The expectation \eqref{eq:is-expectation} can then be written as
\begin{equation*}
\Phi = \int \phi(\bm{x}) \frac{q(\bm{x})}{q(\bm{x})} p(\bm{x}) \; \dx{\bm{x}} = \int \phi(\bm{x}) \underbrace{\frac{p(\bm{x})}{q(\bm{x})}}_{w^*(\bm{x})} q(\bm{x}) \; \dx{\bm{x}} = \E_{q}[\phi(\bm{X}) w^*(\bm{X})],
\end{equation*}
where $w^*(\bm{x})$ are called the importance weights. By defining $w(\bm{x}) = \frac{p^*(\bm{x})}{q^*(\bm{x})}$, $\Phi$ can be approximated by
\begin{equation*}
\Phi \approx \widehat{\Phi} \coloneqq \frac{\sum_{i=1}^N \phi(\bm{x}^{(i)}) w(\bm{x}^{(i)})}{\sum_{i=1}^Nw(\bm{x}^{(i)})}, \quad \bm{x}^{(1)}, \ldots, \bm{x}^{(N)} \stackrel{iid}{\sim} q.
\end{equation*}
We note that by using $w$ instead of $w^*$ and normalizing by the weights sum instead of the sample size $N$, we bypass the evaluation of $Z$ and $Z_Q$, since they cancel out. The importance weights here account for correcting the discrepancy between the distribution $q$ and the true distribution $p$.

The estimator $\widehat{\Phi}$ converges to the true expectation $\Phi$ as $N \to \infty$. However, it is not necessarily unbiased \citep{information-theory}.


\paragraph{Sequential importance sampling (SIS)}
The SIS algorithm uses a weighted set of particles $\left\{\left(\bm{x}_t^{(i)}, w_t^{(i)} \right) : i = 1, \ldots, N \right\}$, to represent the filtering distribution $p(\bm{x}_t \mid \by_{1:t}, \btheta)$. To simplify notation, we write $w_t^{(i)}$ instead of $w_t(\bm{x}^{(i)})$ from now on. The empirical approximation to $p(\bm{x}_t~\mid~\by_{1:t}, \btheta)$ is then
\begin{equation*}
\widehat{p}(\bm{x}_t \mid \by_{1:t}, \btheta) = \frac{\sum_{i=1}^N w_t^{(i)} \delta_{\bm{x}_t^{(i)}}(\bm{x}_t)}{\sum_{i=1}^N w_t^{(i)}}.
\end{equation*}

As the name suggests, the algorithm involves a sequential application of the importance sampling procedure with increasing time $t$.

Returning to the SSM \eqref{sec:ssm-definition}, we consider the posterior distribution of a sequence of states $\bx_{0:t}$ given a sequence of observations $\by_{1:t}$. By application of the Bayes theorem, we obtain the following recursive formula:
\begin{equation*}
\begin{split}
p(\bx_{0:t} \mid \by_{1:t}) & \propto p(\by_t \mid \bx_{0:t}, \by_{1:t-1}) p(\bx_{0:t} \mid \by_{1:t-1}) \\
&= \obs_t(\by_t \mid \bx_t) p(\bx_t \mid \bx_{0:t-1}, \by_{1:t-1}) p(\bx_{0:t-1} \mid \by_{1:t-1}) \\
&= \obs_t(\by_t \mid \bx_t) \trans_t(\bx_t \mid \bx_{t-1}) p(\bx_{0:t-1} \mid \by_{1:t-1}),
\end{split}
\end{equation*}
where the equalities follow from the hidden Markov model independence assumptions. For better clarity, we suppress the static parameter $\btheta$ from the conditioning.

For the target $p(\bx_{0:t} \mid \by_{1:t})$, we introduce the importance sampling distribution $q(\bx_{0:t} \mid \by_{1:t})$ and sample $\bx_{0:t}^{(i)}$ from it. The importance weights are (up to normalization) given by
\begin{equation} \label{eq:weight-recursion1}
\begin{split}
w_t^{(i)} & \propto \frac{p(\bx_{0:t}{(i)} \mid \by_{1:t})}{q(\bx_{0:t}^{(i)} \mid \by_{1:t})} \\
& \propto \frac{\obs_t(\by_t \mid \bx_t^{(i)}) \trans_t(\bx_t^{(i)} \mid \bx_{t-1}^{(i)}) p(\bx_{0:t-1}^{(i)} \mid \by_{1:t-1})}{q(\bx_{0:t}^{(i)} \mid \by_{1:t})}.
\end{split}
\end{equation}
By definition of the conditional probability and the hidden Markov model assumptions, we can write the importance sampling distribution as
\begin{equation*}
q(\bx_{0:t} \mid \by_{1:t}) = q(\bx_t \mid \bx_{0:t-1}, \by_{1:t}) q(\bx_{0:t-1} \mid \by_{1:t-1}).
\end{equation*}
By substituting into \eqref{eq:weight-recursion1}, we obtain the following recursion:
\begin{equation} \label{eq:weight-recursion2}
\begin{split}
w_t^{(i)} & \propto \frac{\obs_t(\by_t \mid \bx_t^{(i)}) \trans_t(\bx_t^{(i)} \mid \bx_{t-1}^{(i)})}{q(\bx_t^{(i)} \mid \bx_{0:t-1}^{(i)}, \by_{1:t})} \frac{p(\bx_{0:t-1}^{(i)} \mid \by_{1:t-1})}{q(\bx_{0:t-1}^{(i)} \mid \by_{1:t-1})} \\
& \propto \frac{\obs_t(\by_t \mid \bx_t^{(i)}) \trans_t(\bx_t^{(i)} \mid \bx_{t-1}^{(i)})}{q(\bx_t^{(i)} \mid \bx_{0:t-1}^{(i)}, \by_{1:t})} w_{t-1}^{(i)}.
\end{split}
\end{equation}
So updating the $i$th weight when transitioning from time $t-1$ to $t$ is a relatively simple task involving only multiplication by the first fraction in \eqref{eq:weight-recursion2}.

The sequential importance sampling algorithm is summarized in \autoref{alg:sis}.
\begin{algorithm}[ht]
    \caption{Sequential Importance Sampling}
    \label{alg:sis}
    \begin{algorithmic}[1]
        \Input $\text{Number of particles } N,\ \text{current parameter value } \btheta,\ \left\{\by_1, \ldots, \by_T\right\}$
        
        \State $\text{Sample } \bx_0^{(i)} \sim \sprior(\cdot \mid \btheta), \quad i = 1, \ldots, N.$ \Comment{Initialize $N$ particles.}
        
        \State $w_0^{(i)} \gets \frac{1}{N}, \quad i = 1, \ldots, N.$ \Comment{Initialize uniform weights.}
        
        \For{$t = 1\ \mathbf{to}\ T$}
            \State $\text{Sample } \bx_t^{(i)} \sim q(\cdot \mid \bx_{0:t-1}^{(i)}, \by_{1:t}, \btheta), \quad i = 1, \ldots, N.$ \Comment{Sample $N$ new particles.}
            \State $\text{Set } w_t^{(i)} \propto \frac{\obs_t(\by_t \mid \bx_t^{(i)}, \btheta) \trans_t(\bx_t^{(i)} \mid \bx_{t-1}^{(i)}, \btheta)}{q(\bx_t^{(i)} \mid \bx_{0:t-1}^{(i)}, \by_{1:t}, \btheta)} w_{t-1}^{(i)}, \quad i = 1, \ldots, N.$ \Comment{Update the weights as per \eqref{eq:weight-recursion2}.}
        \EndFor
    \end{algorithmic}
\end{algorithm}
This by itself is almost the particle filter. There are still two issues to be addressed, though. First, the problem of weight degeneracy discussed in the next paragraph. Second, the choice of the importance sampling distribution $q$ addressed later.


\paragraph{Resampling}
A serious problem preventing the use of the SIS algorithm is that the weights degenerate over time. In each time step, the variance of the weights reduces \citep{particle-filter}. This means that the (normalized) weights always converge to a situation where a single weight is 1 and the others are 0.

To alleviate this, the following resampling step is introduced.
\begin{algorithm}[ht]
    \caption{Multinomial resampling}
    \label{alg:resampling}
    \begin{algorithmic}[1]
        \Input $\text{Importance weights } w_t^{(1)}, \ldots, w_t^{(N)},\ \text{particles } \bx_t^{(1)}, \ldots, \bx_t^{(N)}.$
        
        \State $\widetilde{w}_t^{(i)} \gets \frac{w_t^{(i)}}{\sum_{j=1}^N w_t^{(j)}}, \quad i = 1, \ldots, N.$ \Comment{Normalize the weights.}
        
        \State $\text{Sample } a_i \text{ s.t. } \mathbb{P}(a_i = j) = \widetilde{w}_t^{(j)}, \quad i,j = 1, \ldots, N.$ \Comment{Sample indices with replacement.}
        
        \State $w_t^{(a_i)} \gets \frac{1}{N}, \quad i = 1, \ldots, N.$ \Comment{Reset weights.}
        
        \Output $\text{Resampled particles } \bx_t^{(a_1)}, \ldots, \bx_t^{(a_N)} \text{ and weights } w_t^{(a_1)}, \ldots, w_t^{(a_N)}.$
    \end{algorithmic}
\end{algorithm}

The normalized importance weights are interpreted as a probability vector of a categorical distribution. The particles are then resampled (sampled with replacement) according to this distribution. This effectively selects a population of ``strong individuals'' for the next time step.

The above described procedure is known as multinomial resampling. There are other, more complex, approaches, such as stratified resampling \citep{resampling} which further reduce the variance of the weights, at the cost of added computational complexity.

\paragraph{The particle filter}
The remaining step is the choice of the importance sampling distribution $q$. Obviously, the more similar this distribution is to the target, the closer approximation we obtain.

The particle filter arises when the transition distribution $\trans_t$ is chosen as the importance distribution, that is, when
\begin{equation*}
q(\bx_t \mid \bx_{0:t-1}, \by_{1:t}, \btheta) = \trans_t(\bx_t \mid \bx_{t-1}, \btheta).
\end{equation*}
The importance weights \eqref{eq:weight-recursion2} then simplify into
\begin{equation} \label{eq:weight-recursion3}
w_t^{(i)} \propto \obs_t(\by_t \mid \bx_t^{(i)}) w_{t-1}^{(i)}.
\end{equation}
This form of $q$ is not optimal (the target) distribution. It is however ``close'' to it. The optimal distribution would be $q(\bx_t \mid \bx_{0:t-1}, \by_{1:t}, \btheta) = p(\bx_t \mid \bx_{t-1}, \by_{t}, \btheta)$, due to factorization assumptions of the SSM \eqref{eq:factorization}. Calculating it would involve marginalization over $\bx_t$,
\begin{equation*}
p(\bx_t \mid \bx_{t-1}, \by_{t}, \btheta) = \frac{p(\bx_t, \by_{t} \mid \bx_{t-1}, \btheta)}{\int p(\bx_t, \by_{t} \mid \bx_{t-1}, \btheta) \; \dx{\bx_t}} = \frac{\trans_t(\bx_t \mid \bx_{t-1}, \btheta) \obs_t(\by_t \mid \bx_t, \btheta)}{\int \trans_t(\bx_t \mid \bx_{t-1}, \btheta) \obs_t(\by_t \mid \bx_t, \btheta) \; \dx{\bx_t}},
\end{equation*}
which is generally not possible.

The particle filter is summarized in \autoref{alg:particle-filter}.
\begin{algorithm}[ht]
    \caption{Bootstrap particle filter}
    \label{alg:particle-filter}
    \begin{algorithmic}[1]
        \Input $\text{Number of particles } N,\ \text{current parameter value } \btheta,\ \left\{\by_1, \ldots, \by_T\right\}$
        
        \State $\text{Sample } \bx_0^{(i)} \sim \sprior(\cdot \mid \btheta), \quad i = 1, \ldots, N.$ \Comment{Initialize $N$ particles.}
        
        \State $w_0^{(i)} \gets \frac{1}{N}, \quad i = 1, \ldots, N.$ \Comment{Initialize uniform weights.}
        
        \For{$t = 1\ \mathbf{to}\ T$}
        \State $\text{Sample } \bx_t^{(i)} \sim \trans_t(\bx_t \mid \bx_{t-1}, \btheta), \quad i = 1, \ldots, N.$ \Comment{Sample $N$ new particles.}
        
        \State $\text{Set } w_t^{(i)} \propto \obs_t(\by_t \mid \bx_t^{(i)}, \btheta) w_{t-1}^{(i)}, \quad i = 1, \ldots, N.$ \Comment{Update the weights as per \eqref{eq:weight-recursion3}.}
        
        \State $\text{Resample } \bx_t^{(i)} \text{ and reset } w_t^{(i)} \text{ using \autoref{alg:resampling}}, \quad i = 1, \ldots, N.$
        \EndFor
    \end{algorithmic}
\end{algorithm}
The algorithm is called \emph{bootstrap} particle filter, due to resemblance of the resampling step to the non-parametric bootstrap \citep{bootstrap}.



\section{Using the particle filter to estimate the likelihood} \label{sec:particle-filter-estimate}

As mentioned in \autoref{sec:particle-filter} is typically used to approximate the filtering distribution $p(\bx_t \mid \by_{1:t}, \btheta)$. This will be used to provide a tractable approximation to the likelihood $p(\by_{1:T} \mid \btheta)$ such that it does not affect the limiting distribution of the Markov chain used in the Metropolis-Hastings algorithm. This section describes how it is done, and gives the resulting variant of the Metropolis-Hastings algorithm.

\paragraph{Likelihood estimate in general}
Suppose that we are in possession of an estimator $\widehat{\aux}$ of the likelihood $p(\by_{1:T} \mid \btheta)$. As such, it necessarily depends on $\by_{1:T}$ and $\btheta$. Since we aim to use the particle filter to calculate $\widehat{\aux}$, the estimator also depends on the importance weights. This makes the estimator a random variable with some distribution denoted $\psi(\aux \mid \btheta, \by_{1:T})$. It is not necessary to have this distribution available, as it is later shown to cancel out in the Metropolis-Hastings acceptance ratio.

We now return to our model \eqref{eq:posterior} and introduce $\widehat{\aux}$ as an auxiliary variable, along with our variable of interest $\btheta$. This changes the target distribution from $p(\btheta \mid \by_{1:T})$ to
\begin{equation} \label{eq:psi-joint}
\psi(\btheta, \aux \mid \by_{1:T}) = p(\btheta \mid \by_{1:T}) \psi(\aux \mid \btheta, \by_{1:T}) = \frac{p(\by_{1:T} \mid \btheta) \pprior(\btheta)}{p(\by_{1:T})} \psi(\aux \mid \btheta, \by_{1:T}).
\end{equation}
In theory, we could now construct a Metropolis-Hastings algorithm with $\psi(\btheta, \aux \mid \by_{1:T})$ as the target, instead of $p(\btheta \mid \by_{1:T})$ as was the case in \autoref{alg:metropolis-hastings}. However, this would not solve our problem, since calculating the acceptance ratio still requires the calculation of the likelihood $p(\by_{1:T} \mid \btheta)$, as \eqref{eq:psi-joint} makes clear.

Instead, we define a new target distribution over $(\btheta, \widehat{\aux})$ by replacing the likelihood in \eqref{eq:psi-joint} by its estimate $\widehat{\aux}$:
\begin{equation} \label{eq:aux-joint}
\auxjoint(\btheta, \aux \mid \by_{1:T}) \coloneqq \frac{\aux \pprior(\btheta)}{p(\by_{1:T})} \psi(\aux \mid \btheta, \by_{1:T}).
\end{equation}
There are of course some conditions imposed on $\auxjoint(\btheta, \aux \mid \by_{1:T})$ for it to be useful:
\begin{enumerate}
    \item $\auxjoint(\btheta, \aux \mid \by_{1:T})$ must be non-negative for all $(\btheta, \aux)$;
    \item $\auxjoint(\btheta, \aux \mid \by_{1:T})$ must integrate to 1;
    \item the marginal distribution of $\auxjoint(\btheta, \aux \mid \by_{1:T})$ for $\btheta$ must be the original target $p(\btheta \mid \by_{1:T})$.
\end{enumerate}
The first two conditions simply state that $\auxjoint$ is a valid probability distribution. The third condition ensures that by constructing a Metropolis-Hastings algorithm with $\auxjoint$ as the target, the original target distribution is preserved once the auxiliary variables are marginalized out. All three conditions are satisfied is $\widehat{\aux}$ is a non-negative unbiased estimator of the likelihood $p(\by_{1:T} \mid \btheta)$. This is shown as follows.

\begin{enumerate}[align=left]
    \item Non-negativity of $\auxjoint$ follows from the assumed non-negativity of the estimator $\widehat{\aux}$ and validity of the distributions in \eqref{eq:aux-joint}.
    \item[2, 3.] Assume that $\widehat{\aux}$ is an unbiased estimate of $p(\by_{1:T} \mid \btheta)$, i.e. that $\E_{\psi}[\widehat{\aux}] = p(\by_{1:T} \mid \btheta)$. Consider now the marginal of $\auxjoint$ for $\btheta$:
    \begin{equation} \label{eq:marginal}
    \begin{split}
    \int \auxjoint(\btheta, \aux \mid \by_{1:T})\; \dx{\aux} & = \frac{\pprior(\btheta)}{p(\by_{1:T})} \int \aux \psi(\aux \mid \btheta, \by_{1:T}) \; \dx{\aux} \\
    & = \frac{\pprior(\btheta)}{p(\by_{1:T})} \E_{\psi}[\widehat{\aux}] \\
    & = \frac{\pprior(\btheta)}{p(\by_{1:T})} p(\by_{1:T} \mid \btheta) \\
    & = p(\btheta \mid \by_{1:T}),
    \end{split}
    \end{equation}
    the original target distribution. This satisfies condition 3. For condition 2, we simply integrate \eqref{eq:marginal} w.r.t. $\btheta$, which results in 1 due to $p(\btheta \mid \by_{1:T})$ being a valid probability distribution.
\end{enumerate}

\paragraph{Acceptance ratio computation}
Given the new target distribution $\auxjoint$, we can now construct a Metropolis-Hastings algorithm on the joint space of $(\btheta, \aux)$. This means that the proposal density first samples $\btheta^\prime \sim q(\cdot \mid \btheta^{(m-1)})$, and then $\widehat{\aux}^\prime \sim \psi(\cdot \mid \btheta^\prime, \by_{1:T})$. The acceptance ratio can now be computed as
\begin{equation*}
\begin{split}
\alpha & = \min \left\{1, \frac{\auxjoint(\btheta^\prime, \aux^\prime \mid \by_{1:T})}{\auxjoint(\btheta^{(m-1)}, \aux^{(m-1)} \mid \by_{1:T})} \frac{\prop(\btheta^{(m-1)} \mid \btheta^\prime) \psi(\aux^{(m-1)} \mid \btheta^{(m-1)}, \by_{1:T})}{\prop(\btheta^\prime \mid \btheta^{(m-1)}) \psi(\aux^\prime \mid \btheta^\prime, \by_{1:T})} \right\} \\
& = \min \left\{1, \frac{\aux^\prime \pprior(\btheta^\prime) \psi(\aux^\prime \mid \btheta^\prime, \by_{1:T})}{\aux^{(m-1)} \pprior(\btheta^{(m-1)}) \psi(\aux^{(m-1)} \mid \btheta^{(m-1)}, \by_{1:T})} \frac{\prop(\btheta^{(m-1)} \mid \btheta^\prime) \psi(\aux^{(m-1)} \mid \btheta^{(m-1)}, \by_{1:T})}{\prop(\btheta^\prime \mid \btheta^{(m-1)}) \psi(\aux^\prime \mid \btheta^\prime, \by_{1:T})} \right\} \\
& = \min \left\{1, \frac{\aux^\prime \pprior(\btheta^\prime)}{\aux^{(m-1)} \pprior(\btheta^{(m-1)})} \frac{q(\btheta^{(m-1)} \mid \btheta^\prime)}{q(\btheta^\prime \mid \btheta^{(m-1)})} \right\}.
\end{split}
\end{equation*}

Since \eqref{eq:marginal} shows that the marginal of $\auxjoint$ for $\btheta$ is the original target $p(\btheta \mid \by_{1:T})$, all we need to do is to discard the sampled $\widehat{\aux}^{(m)}$ and keep only $\btheta^{(m)}$ when running Metropolis-Hastings on the joint space of $(\btheta, \aux)$.

\paragraph{Calculating the estimate using the particle filter}
Finally, we describe how exactly is the particle filter used as an estimator of $p(\by_{1:T} \mid \btheta)$.

First, we decompose the likelihood into a product of simpler distributions, which are then marginalized over the corresponding hidden state:
\begin{equation} \label{eq:likelihood-factorization}
\begin{split}
p(\by_{1:T} \mid \btheta) &= \prod_{t=1}^T p(\by_t \mid \by_{1:t-1}, \btheta) \\
&= \prod_{t=1}^T \int p(\by_t, \bx_t \mid \by_{1:t-1}, \btheta) \; \dx{\bx_t} \\
&= \prod_{t=1}^T \int p(\by_t \mid \bx_t, \btheta) p(\bx_t \mid \by_{1:t-1}, \btheta) \; \dx{\bx_t}.
\end{split}
\end{equation}

Using the particles $\left\{\bx_t^{(i)}\right\}_{i=1}^N$, we plug in the empirical approximation to $p(\bx_t \mid \by_{1:t-1}, \btheta)$, $\widehat{p}(\bx_t \mid \by_{1:t-1}, \btheta) = \frac{1}{N} \sum_{i=1}^N \delta_{\bm{x}_t^{(i)}}(\bx_t)$, into \eqref{eq:likelihood-factorization}, obtaining
\begin{equation*}
\begin{split}
p(\by_{1:T} \mid \btheta) & \approx \prod_{t=1}^T \int p(\by_t \mid \bx_t, \btheta) \left[ \frac{1}{N} \sum_{i=1}^N \delta_{\bm{x}_t^{(i)}}(\bx_t) \right] \; \dx{\bx_t} \\
& = \prod_{t=1}^T \frac{1}{N} \sum_{i=1}^N \int p(\by_t \mid \bx_t, \btheta) \delta_{\bm{x}_t^{(i)}}(\bx_t) \; \dx{\bx_t} \\
& = \prod_{t=1}^T \frac{1}{N} \sum_{i=1}^N p(\by_t \mid \bx_t^{(i)}, \btheta)
\end{split}
\end{equation*}
due to linearity of the integral and properties of the Dirac distribution.

In $p(\by_t \mid \bx_t^{(i)}, \btheta)$, we recognize the particle filter weights $w_t^{(i)}$ defined in \eqref{eq:weight-recursion3}. This allows us to finally define the likelihood estimate as
\begin{equation} \label{eq:likelihood-estimate}
\widehat{\aux} \coloneqq \prod_{t=1}^T \frac{1}{N} \sum_{i=1}^N w_t^{(i)}.
\end{equation}
This estimator is obviously non-negative due to construction of the weights. The proof that it is also unbiased (and therefore also integrates to 1) is more involved, and the reader is referred to \cite{del-moral} for the original proof.

Finally, we describe the resulting variant of the Metropolis-Hastings algorithm employing the likelihood estimate \eqref{eq:likelihood-estimate}.

\begin{algorithm}[ht]
    \caption{Marginal Metropolis-Hastings}
    \label{alg:marginal-metropolis-hastings}
    \begin{algorithmic}[1]
        \Input $\text{Number of samples } M,\ \left\{\by_1, \ldots, \by_T\right\}$
        
        \State $\text{Initialize } \btheta^{(0)}.$
        \State $\text{Run \autoref{alg:particle-filter} with } \btheta^{(0)} \text{ to obtain the weights } w_{0,t}^{(i)}, \quad t = 1, \ldots, T,\ i = 1, \ldots, N.$
        \State $\text{Calculate } \widehat{\aux}^{(0)} \text{ according to \eqref{eq:likelihood-estimate} using } w_{0,t}^{(i)}.$
        
        \For{$m = 1\ \mathbf{to}\ M$}
        \State $\text{Sample } \btheta^\prime \sim \prop(\cdot \mid \btheta^{(m-1)}).$
        \State $\text{Run \autoref{alg:particle-filter} with } \btheta^\prime \text{ to obtain the weights } w_{m,t}^{(i)}, \quad t = 1, \ldots, T, \ i = 1, \ldots, N.$
        \State $\text{Calculate } \widehat{\aux}^\prime \text{ according to \eqref{eq:likelihood-estimate} using } w_{m,t}^{(i)}.$
        \State $\text{Calculate the aceptance probability } $ \begin{equation} \label{eq:acceptance-probability-tractable}
        \alpha = \min \left\{1, \frac{\widehat{\aux}^\prime \pprior(\btheta^\prime)}{\widehat{\aux}^{(m-1)} \pprior(\btheta^{(m-1)})} \frac{\prop(\btheta^{(m-1)} \mid \btheta^\prime)}{\prop(\btheta^\prime \mid \btheta^{(m-1)})} \right\}.
        \end{equation}
        \State $\text{Sample } u \sim \mathcal{U}(0,1).$
        \If {$u \leq \alpha$}
        \State $\left( \btheta^{(m)}, \widehat{\aux}^{(m)} \right) \gets \left( \btheta^\prime, \widehat{\aux}^\prime \right)$ \Comment{With probability $\alpha$, accept the proposed sample.}
        \Else
        \State $\left( \btheta^{(m)}, \widehat{\aux}^{(m)} \right) \gets \left( \btheta^{(m-1)}, \widehat{\aux}^{(m-1)} \right)$ \Comment{With probability $1 - \alpha$, reject the proposed sample.}
        \EndIf
        \EndFor
        
        \Output $\left\{ \btheta^{(1)}, \ldots, \btheta^{(M)} \right\}$
    \end{algorithmic}
\end{algorithm}

This algorithm, called marginal Metropolis-Hastings, was introduced in \cite{andrieu}. Compared to \autoref{alg:metropolis-hastings}, all components of this algorithm can be evaluated. Due to construction of the estimator $\widehat{\aux}$, the marginal of the limiting distribution of \autoref{alg:marginal-metropolis-hastings} is the original target $p(\btheta \mid \by_{1:T})$.